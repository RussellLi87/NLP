{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Clean Text Manually and with NLTK**\n",
    "\n",
    "**1. Text Cleaning Is Task Specific**\n",
    "\n",
    "**2. Manual Tokenization**\n",
    "\n",
    "Text cleaning is hard, but the text we have chosen to work with is pretty clean already. We\n",
    "could just write some Python code to clean it up manually, and this is a good exercise for those\n",
    "simple problems that you encounter. Tools like regular expressions and splitting strings can get\n",
    "you a long way.\n",
    "\n",
    "**2.1 Load Data**\n",
    "\n",
    "Let's load the text data so that we can work with it. The text is small and will load quickly\n",
    "and easily fit into memory. This will not always be the case and you may need to write code\n",
    "to memory map the file. Tools like NLTK (covered in the next section) will make working\n",
    "with large files much easier. We can load the entire metamorphosis clean.txt into memory as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Split by Whitespace**\n",
    "\n",
    "Clean text often means a list of words or tokens that we can work with in our machine learning\n",
    "models. This means converting the raw text into a list of words and saving it again. A very\n",
    "simple way to do this would be to split the document by white space, including \" \" (space), new\n",
    "lines, tabs and more. We can do this in Python with the split() function on the loaded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text \n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Select Words**\n",
    "\n",
    "Another approach might be to use the regex model (re) and split the document into words by\n",
    "selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and '_'). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, running the example we can see that we get our list of words. This time, we can see\n",
    "that armour-like is now two words armour and like (fine) but contractions like What's is also\n",
    "two words What and s (not great).\n",
    "\n",
    "**2.4 Split by Whitespace and Remove Punctuation**\n",
    "\n",
    "We may want the words, but without the punctuation like commas and quotes. We also want to\n",
    "keep contractions together. One way would be to split the document into words by white space\n",
    "(as in the section Split by Whitespace), then use string translation to replace all punctuation with\n",
    "nothing (e.g. remove it). Python provides a constant called string.punctuation that provides a\n",
    "great list of punctuation characters. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regular expressions to select for the punctuation characters and use the sub()\n",
    "function to replace them with nothing. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[abcdefghijklmnopqrstuvwxyz0123456789!\\#\\$%\\&'\\*\\+\\-\\.\\^_`\\|\\~:]+\n"
     ]
    }
   ],
   "source": [
    "operators = ['c', 'd', 'a', 'b', 'e']\n",
    "\n",
    "print('|'.join(map(re.escape, sorted(operators, reverse=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Tokenization and Cleaning with NLTK**\n",
    "\n",
    "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and\n",
    "modeling text. It provides good tools for loading and cleaning text that we can use to get our\n",
    "data ready for working with machine learning and deep learning algorithms.\n",
    "\n",
    "**3.1 Install NLTK**\n",
    "\n",
    "After installation, you will need to install the data used with the library, including a great\n",
    "set of documents that you can use later for testing other tools in NLTK. There are few ways to\n",
    "do this, such as from within a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from the command line:\n",
    "\n",
    "python -m nltk.downloader all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

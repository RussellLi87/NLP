{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Clean Text Manually and with NLTK**\n",
    "\n",
    "**1. Text Cleaning Is Task Specific**\n",
    "\n",
    "**2. Manual Tokenization**\n",
    "\n",
    "Text cleaning is hard, but the text we have chosen to work with is pretty clean already. We\n",
    "could just write some Python code to clean it up manually, and this is a good exercise for those\n",
    "simple problems that you encounter. Tools like regular expressions and splitting strings can get\n",
    "you a long way.\n",
    "\n",
    "**2.1 Load Data**\n",
    "\n",
    "Let's load the text data so that we can work with it. The text is small and will load quickly\n",
    "and easily fit into memory. This will not always be the case and you may need to write code\n",
    "to memory map the file. Tools like NLTK (covered in the next section) will make working\n",
    "with large files much easier. We can load the entire metamorphosis clean.txt into memory as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Split by Whitespace**\n",
    "\n",
    "Clean text often means a list of words or tokens that we can work with in our machine learning\n",
    "models. This means converting the raw text into a list of words and saving it again. A very\n",
    "simple way to do this would be to split the document by white space, including \" \" (space), new\n",
    "lines, tabs and more. We can do this in Python with the split() function on the loaded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text \n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Select Words**\n",
    "\n",
    "Another approach might be to use the regex model (re) and split the document into words by\n",
    "selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and '_'). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, running the example we can see that we get our list of words. This time, we can see\n",
    "that armour-like is now two words armour and like (fine) but contractions like What's is also\n",
    "two words What and s (not great).\n",
    "\n",
    "**2.4 Split by Whitespace and Remove Punctuation**\n",
    "\n",
    "We may want the words, but without the punctuation like commas and quotes. We also want to\n",
    "keep contractions together. One way would be to split the document into words by white space\n",
    "(as in the section Split by Whitespace), then use string translation to replace all punctuation with\n",
    "nothing (e.g. remove it). Python provides a constant called string.punctuation that provides a\n",
    "great list of punctuation characters. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regular expressions to select for the punctuation characters and use the sub()\n",
    "function to replace them with nothing. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[abcdefghijklmnopqrstuvwxyz0123456789!\\#\\$%\\&'\\*\\+\\-\\.\\^_`\\|\\~:]+\n"
     ]
    }
   ],
   "source": [
    "operators = ['c', 'd', 'a', 'b', 'e']\n",
    "\n",
    "print('|'.join(map(re.escape, sorted(operators, reverse=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Tokenization and Cleaning with NLTK**\n",
    "\n",
    "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and\n",
    "modeling text. It provides good tools for loading and cleaning text that we can use to get our\n",
    "data ready for working with machine learning and deep learning algorithms.\n",
    "\n",
    "**3.1 Install NLTK**\n",
    "\n",
    "After installation, you will need to install the data used with the library, including a great\n",
    "set of documents that you can use later for testing other tools in NLTK. There are few ways to\n",
    "do this, such as from within a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from the command line:\n",
    "python -m nltk.downloader all\n",
    "\n",
    "**3.2 Split into Sentences**\n",
    "\n",
    "A good useful first step is to split the text into sentences. Some modeling tasks prefer input\n",
    "to be in the form of paragraphs or sentences, such as Word2Vec. You could first split your\n",
    "text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
    "NLTK provides the sent tokenize() function to split text into sentences. The example below\n",
    "loads the metamorphosis clean.txt file into memory, splits it into sentences, and prints the\n",
    "first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Split into Words**\n",
    "\n",
    "NLTK provides a function called word tokenize() for splitting strings into tokens (nominally\n",
    "words). It splits tokens based on white space and punctuation. For example, commas and\n",
    "periods are taken as separate tokens. Contractions are split apart (e.g. What's becomes What\n",
    "and 's). Quotes are kept, and so on. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Filter Out Punctuation**\n",
    "\n",
    "We can filter out all tokens that we are not interested in, such as all standalone punctuation. This\n",
    "can be done by iterating over all tokens and only keeping those tokens that are all alphabetic.\n",
    "Python has the function isalpha() that can be used. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwords = [word for word in tokens if not word.isalpha()]\\nprint(words[:100])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n",
    "\n",
    "'''\n",
    "words = [word for word in tokens if not word.isalpha()]\n",
    "print(words[:100])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Filter out Stop Words (and Pipeline)**\n",
    "\n",
    "Stop words are those words that do not contribute to the deeper meaning of the phrase. They\n",
    "are the most common words such as: the, a, and is. For some applications like documentation\n",
    "classification, it may make sense to remove stop words. NLTK provides a list of commonly\n",
    "agreed upon stop words for a variety of languages, such as English. They can be loaded as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that they are all lower case and have punctuation removed. You could compare\n",
    "your tokens to the stop words and filter them out, but you must ensure that your text is prepared\n",
    "the same way. Let's demonstrate this with a small pipeline of text preparation including:\n",
    "- Load the raw text.\n",
    "- Split into tokens.\n",
    "- Convert to lowercase.\n",
    "- Remove punctuation from each token.\n",
    "- Filter out remaining tokens that are not alphabetic.\n",
    "- Filter out tokens that are stop words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
